{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "from ptsa.utils.utility import _get_sklearn_version\n",
    "\n",
    "if _get_sklearn_version() > 20:\n",
    "    from inspect import signature\n",
    "else:\n",
    "    from sklearn.externals.funcsigs import signature\n",
    "\n",
    "import abc\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "from scipy.special import erf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import deprecated\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "\n",
    "from sklearn_base import _pprint\n",
    "from ptsa.utils.utility import precision_n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@six.add_metaclass(abc.ABCMeta)\n",
    "class BaseDetector(object):\n",
    "    \"\"\"Abstract class for type A anomoly detection algorithms.\n",
    "    Parameters\n",
    "    ----------\n",
    "    contamination : float, optional (default=0.1)\n",
    "        The proportion of anomolies of the data set,\n",
    "        Used for defining the threshold on the decision function.\n",
    "    window: int, optional (default = 0.1)\n",
    "        The length of the window to detect the given anomolies \n",
    "    Attributes\n",
    "    ----------\n",
    "    decision_scores_ : numpy array of shape (n_samples,)\n",
    "        The outlier scores of the training data.\n",
    "        The higher, the more abnormal. Outliers tend to have higher\n",
    "        scores. This value is available once the detector is fitted.\n",
    "    threshold_ : float\n",
    "        The threshold is based on ``contamination``. It is the\n",
    "        ``n_samples * contamination`` most abnormal samples in\n",
    "        ``decision_scores_``. The threshold is calculated for generating\n",
    "        binary outlier labels.\n",
    "    labels_ : int, either 0 or 1\n",
    "        The binary labels of the training data. 0 stands for inliers\n",
    "        and 1 for outliers/anomalies. It is generated by applying\n",
    "        ``threshold_`` on ``decision_scores_``.\n",
    "    \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, contamination=0.1, window = 75):\n",
    "\n",
    "        if not (0. < contamination <= 0.55):\n",
    "            raise ValueError(\"contamination must be in (0, 0.55], \"\n",
    "                             \"got: %f\" % contamination)\n",
    "\n",
    "        self.contamination = contamination\n",
    "\n",
    "    # noinspection PyIncorrectDocstring\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit detector. y is ignored in unsupervised methods.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        y : Ignored\n",
    "            Not used, present for API consistency by convention.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def decision_function(self, X, measure = None):\n",
    "        \"\"\"Predict raw anomaly scores of X using the fitted detector and distance measure\n",
    "        The anomaly score of an input sample is computed based on the fitted\n",
    "        detector. For consistency, outliers are assigned with\n",
    "        higher anomaly scores.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples. Sparse matrices are accepted only\n",
    "            if they are supported by the base estimator.\n",
    "        measre : function that maps two 1d numpy array to integer. \n",
    "            Using euclidean measure if not specified. \n",
    "        Returns\n",
    "        -------\n",
    "        anomaly_scores : numpy array of shape (n_samples,)\n",
    "            The anomaly score of the input samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict if a particular sample is an outlier or not.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        Returns\n",
    "        -------\n",
    "        outlier_labels : numpy array of shape (n_samples,)\n",
    "            For each observation, tells whether or not\n",
    "            it should be considered as an outlier according to the\n",
    "            fitted model. 0 stands for inliers and 1 for outliers.\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])\n",
    "\n",
    "        pred_score = self.decision_function(X)\n",
    "        return (pred_score > self.threshold_).astype('int').ravel()\n",
    "\n",
    "    def predict_proba(self, X, method='linear'):\n",
    "        \"\"\"Predict the probability of a sample being outlier. Two approaches\n",
    "        are possible:\n",
    "        1. simply use Min-max conversion to linearly transform the outlier\n",
    "           scores into the range of [0,1]. The model must be\n",
    "           fitted first.\n",
    "        2. use unifying scores, see :cite:`kriegel2011interpreting`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        method : str, optional (default='linear')\n",
    "            probability conversion method. It must be one of\n",
    "            'linear' or 'unify'.\n",
    "        Returns\n",
    "        -------\n",
    "        outlier_probability : numpy array of shape (n_samples,)\n",
    "            For each observation, tells whether or not\n",
    "            it should be considered as an outlier according to the\n",
    "            fitted model. Return the outlier probability, ranging\n",
    "            in [0,1].\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self, ['decision_scores_', 'threshold_', 'labels_'])\n",
    "        train_scores = self.decision_scores_\n",
    "\n",
    "        test_scores = self.decision_function(X)\n",
    "\n",
    "        probs = np.zeros([X.shape[0], int(self._classes)])\n",
    "        if method == 'linear':\n",
    "            scaler = MinMaxScaler().fit(train_scores.reshape(-1, 1))\n",
    "            probs[:, 1] = scaler.transform(\n",
    "                test_scores.reshape(-1, 1)).ravel().clip(0, 1)\n",
    "            probs[:, 0] = 1 - probs[:, 1]\n",
    "            return probs\n",
    "\n",
    "        elif method == 'unify':\n",
    "            # turn output into probability\n",
    "            pre_erf_score = (test_scores - self._mu) / (\n",
    "                    self._sigma * np.sqrt(2))\n",
    "            erf_score = erf(pre_erf_score)\n",
    "            probs[:, 1] = erf_score.clip(0, 1).ravel()\n",
    "            probs[:, 0] = 1 - probs[:, 1]\n",
    "            return probs\n",
    "        else:\n",
    "            raise ValueError(method,\n",
    "                             'is not a valid probability conversion method')\n",
    "\n",
    "    def _predict_rank(self, X, normalized=False):\n",
    "        \"\"\"Predict the outlyingness rank of a sample by a fitted model. The\n",
    "        method is for outlier detector score combination.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "        normalized : bool, optional (default=False)\n",
    "            If set to True, all ranks are normalized to [0,1].\n",
    "        Returns\n",
    "        -------\n",
    "        ranks : array, shape (n_samples,)\n",
    "            Outlying rank of a sample according to the training data.\n",
    "        \"\"\"\n",
    "\n",
    "        check_is_fitted(self, ['decision_scores_'])\n",
    "\n",
    "        test_scores = self.decision_function(X)\n",
    "        train_scores = self.decision_scores_\n",
    "\n",
    "        sorted_train_scores = np.sort(train_scores)\n",
    "        ranks = np.searchsorted(sorted_train_scores, test_scores)\n",
    "\n",
    "        if normalized:\n",
    "            # return normalized ranks\n",
    "            ranks = ranks / ranks.max()\n",
    "        return ranks\n",
    "\n",
    " \n",
    "\n",
    "    def _set_n_classes(self, y):\n",
    "        \"\"\"Set the number of classes if `y` is presented, which is not\n",
    "        expected. It could be useful for multi-class outlier detection.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array of shape (n_samples,)\n",
    "            Ground truth.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        self._classes = 2  # default as binary classification\n",
    "        if y is not None:\n",
    "            check_classification_targets(y)\n",
    "            self._classes = len(np.unique(y))\n",
    "            warnings.warn(\n",
    "                \"y should not be presented in unsupervised learning.\")\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def _process_decision_scores(self):\n",
    "        \"\"\"Internal function to calculate key attributes:\n",
    "        - threshold_: used to decide the binary label\n",
    "        - labels_: binary labels of training data\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        self.threshold_ = percentile(self.decision_scores_,\n",
    "                                     100 * (1 - self.contamination))\n",
    "        self.labels_ = (self.decision_scores_ > self.threshold_).astype(\n",
    "            'int').ravel()\n",
    "\n",
    "        # calculate for predict_proba()\n",
    "\n",
    "        self._mu = np.mean(self.decision_scores_)\n",
    "        self._sigma = np.std(self.decision_scores_)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # noinspection PyMethodParameters\n",
    "    def _get_param_names(cls):\n",
    "        # noinspection PyPep8\n",
    "        \"\"\"Get parameter names for the estimator\n",
    "        See http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html\n",
    "        and sklearn/base.py for more information.\n",
    "        \"\"\"\n",
    "\n",
    "        # fetch the constructor or the original constructor before\n",
    "        # deprecation wrapping if any\n",
    "        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n",
    "        if init is object.__init__:\n",
    "            # No explicit constructor to introspect\n",
    "            return []\n",
    "\n",
    "        # introspect the constructor arguments to find the model parameters\n",
    "        # to represent\n",
    "        init_signature = signature(init)\n",
    "        # Consider the constructor parameters excluding 'self'\n",
    "        parameters = [p for p in init_signature.parameters.values()\n",
    "                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n",
    "        for p in parameters:\n",
    "            if p.kind == p.VAR_POSITIONAL:\n",
    "                raise RuntimeError(\"scikit-learn estimators should always \"\n",
    "                                   \"specify their parameters in the signature\"\n",
    "                                   \" of their __init__ (no varargs).\"\n",
    "                                   \" %s with constructor %s doesn't \"\n",
    "                                   \" follow this convention.\"\n",
    "                                   % (cls, init_signature))\n",
    "        # Extract and sort argument names excluding 'self'\n",
    "        return sorted([p.name for p in parameters])\n",
    "\n",
    "    # noinspection PyPep8\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator.\n",
    "        See http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html\n",
    "        and sklearn/base.py for more information.\n",
    "        Parameters\n",
    "        ----------\n",
    "        deep : bool, optional (default=True)\n",
    "            If True, will return the parameters for this estimator and\n",
    "            contained subobjects that are estimators.\n",
    "        Returns\n",
    "        -------\n",
    "        params : mapping of string to any\n",
    "            Parameter names mapped to their values.\n",
    "        \"\"\"\n",
    "\n",
    "        out = dict()\n",
    "        for key in self._get_param_names():\n",
    "            # We need deprecation warnings to always be on in order to\n",
    "            # catch deprecated param values.\n",
    "            # This is set in utils/__init__.py but it gets overwritten\n",
    "            # when running under python3 somehow.\n",
    "            warnings.simplefilter(\"always\", DeprecationWarning)\n",
    "            try:\n",
    "                with warnings.catch_warnings(record=True) as w:\n",
    "                    value = getattr(self, key, None)\n",
    "                if len(w) and w[0].category == DeprecationWarning:\n",
    "                    # if the parameter is deprecated, don't show it\n",
    "                    continue\n",
    "            finally:\n",
    "                warnings.filters.pop(0)\n",
    "\n",
    "            # XXX: should we rather test if instance of estimator?\n",
    "            if deep and hasattr(value, 'get_params'):\n",
    "                deep_items = value.get_params().items()\n",
    "                out.update((key + '__' + k, val) for k, val in deep_items)\n",
    "            out[key] = value\n",
    "        return out\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        # noinspection PyPep8\n",
    "        \"\"\"Set the parameters of this estimator.\n",
    "        The method works on simple estimators as well as on nested objects\n",
    "        (such as pipelines). The latter have parameters of the form\n",
    "        ``<component>__<parameter>`` so that it's possible to update each\n",
    "        component of a nested object.\n",
    "        See http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html\n",
    "        and sklearn/base.py for more information.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "\n",
    "        if not params:\n",
    "            # Simple optimization to gain speed (inspect is slow)\n",
    "            return self\n",
    "        valid_params = self.get_params(deep=True)\n",
    "\n",
    "        nested_params = defaultdict(dict)  # grouped by prefix\n",
    "        for key, value in params.items():\n",
    "            key, delim, sub_key = key.partition('__')\n",
    "            if key not in valid_params:\n",
    "                raise ValueError('Invalid parameter %s for estimator %s. '\n",
    "                                 'Check the list of available parameters '\n",
    "                                 'with `estimator.get_params().keys()`.' %\n",
    "                                 (key, self))\n",
    "\n",
    "            if delim:\n",
    "                nested_params[key][sub_key] = value\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "\n",
    "        for key, sub_params in nested_params.items():\n",
    "            valid_params[key].set_params(**sub_params)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "        # noinspection PyPep8\n",
    "        \"\"\"\n",
    "        See http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html\n",
    "        and sklearn/base.py for more information.\n",
    "        \"\"\"\n",
    "\n",
    "        class_name = self.__class__.__name__\n",
    "        return '%s(%s)' % (class_name, _pprint(self.get_params(deep=False),\n",
    "                                               offset=len(class_name), ),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class BaseDetector with abstract methods __init__, decision_function, fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ac85a1d8f9ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mABD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class BaseDetector with abstract methods __init__, decision_function, fit"
     ]
    }
   ],
   "source": [
    "ABD = BaseDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Polynomial Anomoly Detector with GARCH method and raw error method\n",
    "\"\"\"\n",
    "# Author: Yinchen Wu <yinchen@uchicago.edu>\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from arch import arch_model\n",
    "import pandas as pd\n",
    "import math\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "import os\n",
    "import dis\n",
    "import statistics\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "\n",
    "class Poly:\n",
    "    \"\"\"An elementary method to detect pointwise anomolies using polynomial approxiamtion. \n",
    "    A polynomial of certain degree and window size is fitted to the given timeseries dataset.\n",
    "    A GARCH method is ran on the difference betweeen the approximation and the true value of \n",
    "    the dataset to estimate the volatitilies of each point. A detector score is derived on each \n",
    "    point based on the estimated volatitilies and residual to measure the normality of each point.\n",
    "    An alternative method that only considers absoulte difference is also used. \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : int, optional (default=100)\n",
    "        The number of base estimators in the ensemble.\n",
    "    max_samples : int or float, optional (default=\"auto\")\n",
    "        The number of samples to draw from X to train each base estimator.\n",
    "            - If int, then draw `max_samples` samples.\n",
    "            - If float, then draw `max_samples * X.shape[0]` samples.\n",
    "            - If \"auto\", then `max_samples=min(256, n_samples)`.\n",
    "    contamination : float in (0., 0.5), optional (default=0.1)\n",
    "        The amount of contamination of the data set, i.e. the proportion\n",
    "        of outliers in the data set. Used when fitting to define the threshold\n",
    "        on the decision function.\n",
    "    max_features : int or float, optional (default=1.0)\n",
    "        The number of features to draw from X to train each base estimator.\n",
    "            - If int, then draw `max_features` features.\n",
    "            - If float, then draw `max_features * X.shape[1]` features.\n",
    "            - this attribute is useless of a sensor timeseries anomly\n",
    "    bootstrap : bool, optional (default=False)\n",
    "        If True, individual trees are fit on random subsets of the training\n",
    "        data sampled with replacement. If False, sampling without replacement\n",
    "        is performed.\n",
    "    n_jobs : integer, optional (default=1)\n",
    "        The number of jobs to run in parallel for both `fit` and `predict`.\n",
    "        If -1, then the number of jobs is set to the number of cores.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    Attributes\n",
    "    ----------\n",
    "    estimators_ : list of DecisionTreeClassifier\n",
    "        The collection of fitted sub-estimators.\n",
    "    estimators_samples_ : list of arrays\n",
    "        The subset of drawn samples (i.e., the in-bag samples) for each base\n",
    "        estimator.\n",
    "    max_samples_ : integer\n",
    "        The actual number of samples\n",
    "    decision_scores_ : numpy array of shape (n_samples,)\n",
    "        The outlier scores of the training data.\n",
    "        The higher, the more abnormal. Outliers tend to have higher\n",
    "        scores. This value is available once the detector is\n",
    "        fitted.\n",
    "    threshold_ : float\n",
    "        The threshold is based on ``contamination``. It is the\n",
    "        ``n_samples * contamination`` most abnormal samples in\n",
    "        ``decision_scores_``. The threshold is calculated for generating\n",
    "        binary outlier labels.\n",
    "    labels_ : int, either 0 or 1\n",
    "        The binary labels of the training data. 0 stands for inliers\n",
    "        and 1 for outliers/anomalies. It is generated by applying\n",
    "        ``threshold_`` on ``decision_scores_``.\n",
    "    \"\"\"\n",
    "    def __init__(self, power = 0, ep=3, window = 100, method='GARCH', p=1, q=1):\n",
    "        self.method = method\n",
    "        self.power = power\n",
    "        self.window = window\n",
    "        self.order = (p,q)\n",
    "        self.ep = ep\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit detector. y is ignored in unsupervised methods.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape (n_samples, )\n",
    "            The input samples.\n",
    "        y : Ignored\n",
    "            Not used, present for API consistency by convention.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        # validate inputs X and y (optional)\n",
    "        self._set_n_classes(y)\n",
    "\n",
    "        self.X_train_ = X\n",
    "        self.n_train_ = len(X)\n",
    "        self.decision_scores_ = np.zeros([self.n_train_, 1])\n",
    "\n",
    "        self.approximate()\n",
    "        Y = self.residual\n",
    "\n",
    "        if self.method == 'GARCH':\n",
    "            self._fit_garch(Y)\n",
    "        elif self.method == 'Error':\n",
    "            self._fit_error(Y)\n",
    "        else:\n",
    "            raise ValueError(self.method, \"is not a valid method\")\n",
    "\n",
    "        # flip the scores\n",
    "        #self.decision_scores_ = self.decision_scores_.ravel() * -1\n",
    "        #self._process_decision_scores()\n",
    "        return self\n",
    "\n",
    "    def approximate(self):\n",
    "        '''Approximate the timeseries by polynomials \n",
    "        '''\n",
    "        window = self.window\n",
    "        N = math.floor(self.n_train_ / window)\n",
    "        data = self.X_train_\n",
    "        power=self.power\n",
    "        fit = {}\n",
    "        for i in range(N):\n",
    "            x = np.arange(i*window, (i+1)*window)\n",
    "            y = data[i * window : (i+1)*window]\n",
    "            mymodel = np.poly1d(np.polyfit(x, y, power))\n",
    "            fit['model' + str(i)] = mymodel\n",
    "        if self.n_train_  % N != 0:\n",
    "            x = np.arange(N*window, self.n_train_ )\n",
    "            y = data[N*window :]\n",
    "            mymodel = np.poly1d(np.polyfit(x, y, power))\n",
    "            fit['model'+str(N+1)] = mymodel\n",
    "    \n",
    "        Y = np.zeros(self.n_train_ )\n",
    "        for i in range(N):\n",
    "            myline = np.linspace(window *i, window * (i+1), window)\n",
    "            Y[window *i: window * (i+1)] = data[window *i: window * (i+1)] - fit['model' + str(i)](myline)\n",
    "        if self.n_train_ % N != 0:\n",
    "            x = np.arange(N*window, self.n_train_ )\n",
    "            Y[N*window:] = data[N*window:] - fit['model'+str(N+1)](x)  \n",
    "        self.residual = Y\n",
    "        self.approxiamtion = data - Y  \n",
    "        self.params = fit\n",
    "        return self\n",
    "\n",
    "    \n",
    "    def _fit_garch(self, Y):\n",
    "        \"\"\"Detect the errors based on GARCH method\n",
    "        \"\"\"\n",
    "        pair = self.order \n",
    "        p = pair[0]\n",
    "        q = pair[1]\n",
    "        model = arch_model(Y, mean='Zero', vol='GARCH', p=p, q=q)\n",
    "        model_fit = model.fit(disp='off')\n",
    "        result = np.abs(Y)/ (6* model_fit.conditional_volatility)\n",
    "        result = np.minimum(result, np.ones(len(result)))\n",
    "        self.decision_scores_ = result\n",
    "        return self\n",
    "\n",
    "    def _fit_error(self, Y):\n",
    "        \"\"\"Detect the errors based on GARCH method\n",
    "        \"\"\"\n",
    "        window = self.window\n",
    "        fit = self.params\n",
    "        result = np.zeros(self.n_train_)\n",
    "        for i in range(self.n_train_):\n",
    "            N = math.floor(i / window)\n",
    "            error = (np.max(data[window *N: window * (N+1)])- np.min(data[window *N: window * (N+1)]))\n",
    "            if  'model' + str(N) in fit:\n",
    "                Y= abs(data[i] - fit['model' + str(N)](i))\n",
    "                if Y == 0:\n",
    "                    result[i] = 0\n",
    "                else:\n",
    "                    result[i] = min(Y/(0.5*error), 1)\n",
    "        self.decision_scores_ = result\n",
    "        return self\n",
    "\n",
    "    def _set_n_classes(self, y):\n",
    "        \"\"\"Set the number of classes if `y` is presented for multi-class outlier detection. If \n",
    "        y is not presented, nummber of classes is set to 2.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array of shape (n_samples,)\n",
    "            Ground truth.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        self._classes = 2  # default as binary classification\n",
    "        if y is not None:\n",
    "            sklearn.utils.multiclass.check_classification_targets(y)\n",
    "            self._classes = len(np.unique(y))\n",
    "            warnings.warn(\n",
    "                \"y should not be presented in unsupervised learning.\")\n",
    "        return self   \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
